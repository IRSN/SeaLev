%% -*- fill-column: 80; comment-column: 50; -*-
\documentclass[11pt,a4paper]{report}

% \VignetteIndexEntry{Introduction to SeaLev}
% \VignetteDepends{SeaLev}
% \VignetteKeyword{sea level convolution}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage[english]{babel}

\usepackage{url}
\usepackage{fullpage}
%%\usepackage{natbib}
\usepackage{boxedminipage}
\usepackage{hyperref}
\usepackage{makeidx}
\usepackage{titlesec}
\usepackage{color}
\makeindex

%% \parskip=1.5ex plus 1.5ex minus 1.25ex
%% \titleformat{\section}[block]{\normalfont\large\bfseries}{\thesection}{1em}{}
%% \titlespacing{\section}{0em}{2em plus 3em minus 0.5em}{0.15em plus 0.15em
%%   minus 0.125em}
%% \titleformat{\subsection}[block]{\normalfont\large\itshape}{\thesubsection}{1em}{}
%% \titlespacing{\subsection}{0em}{1em plus 2em minus 0.5em}{-0.15em plus 0.15em
%%   minus 0.125em}


%% commands
\makeatletter
\newcommand\code{\bgroup\@makeother\_\@makeother\~\@makeother\$\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}\egroup}
\makeatother
%%\let\code=\texttt
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}

%%======================================================== 
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\m}{\mathbf}   
\newcommand{\bs}{\boldsymbol}
\newcommand{\pCond}[2]{\left( #1 \;\middle\vert\; #2 \right)}
\newcommand{\bCond}[2]{\left[ #1 \;\middle\vert\; #2 \right]}
\newcommand{\Cond}[2]{\left. #1 \;\middle\vert\; #2 \right.}
\newcommand{\Low}[1]{#1_{\mathrm{min}}}
\newcommand{\Up}[1]{#1_{\mathrm{max}}}

%%========================================================= 
\definecolor{InputColor}{rgb}{0.600,0.060,0.360} 
\definecolor{OutputColor}{rgb}{0.133,0.543,0.133}
\definecolor{Gray}{rgb}{0.5,0.5,0.5}
%%=========================================================
\newenvironment{remark}
   {\medskip \par \noindent% 
    \textbf{Remark}.%
   }%
   {\par \noindent}
%%=========================================================
\newenvironment{remarks}
   {\medskip \par \noindent% 
    \textbf{Remarks}\\\vspace{-1.5em}\nopagebreak%
    \begin{list}{$\bullet$}{%
       \setlength{\labelwidth}{-6pt}%
       \setlength{\leftmargin}{0pt}%
       \setlength{\rightmargin}{0pt}%
     }
   }%
   {\end{list} \par \noindent}
%%=========================================================
\newenvironment{Prov}
   {\medskip \par \noindent%
    \sf \color{blue} }%
  {\medskip \par}
%%=========================================================
\title{
  \begin{tabular}{c}
  \noalign{\hrule height 2pt}
   {\Huge The \textbf{SeaLev} package \rule{0pt}{2em}}\\
   version 0.3-0 \\ 
   \noalign{\hrule height 2pt}
\end{tabular}
}

\author{Yves Deville\rule{0pt}{140pt}}


\begin{document}
\pagenumbering{roman}   % i, ii, iii, iv, ...

\thispagestyle{empty}

\begin{center}
  \rule{0pt}{200pt}
  \begin{tabular}{c}
    \noalign{\hrule height 1pt}
    {\Huge \sf the \textbf{SeaLev} package\rule{0pt}{1em}}\\
    {\huge \sf user guide} \rule[-12pt]{0pt}{32pt}\\ 
    \noalign{\hrule height 1pt}
  \end{tabular}
  
 
  \begin{tabular}{c}
    {\Large \sf Yves Deville}\rule{0pt}{5cm}\\
    {\large \sf \today, SeaLev version \Sexpr{packageVersion("SeaLev")}} \rule{0pt}{5cm}\\
  \end{tabular}

\end{center}

\pagebreak
















%% first page: copyright only
\thispagestyle{empty}
\rule{0pt}{\textheight}
\begin{tabular}{l}
  %%\hline
  \noalign{\hrule height 2pt}
  Copyright \copyright \: 2010, 2013, 2016 IRSN-Yves Deville\rule{0pt}{12pt}
\end{tabular}

\pagebreak
\setcounter{page}{1}
\tableofcontents

%%\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
%%\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
%%\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}

%% Customisation as suggested by Ross Ihaka: indent, etc.

\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\DefineVerbatimEnvironment{Sinput}{Verbatim}{framesep=2pt, %
  xleftmargin=2em, %
  formatcom = {\color{InputColor}\small}%
}%
\DefineVerbatimEnvironment{Soutput}{Verbatim}{framesep=2pt, %
  %frame=lines,
  xleftmargin=2em, %
  formatcom = {\color{OutputColor}\small}%
}%

\SweaveOpts{prefix.string=Rgraphics/fig, eps=FALSE, pdf=TRUE, keep.source=TRUE}
\setkeys{Gin}{width=7cm}
<<options, echo = FALSE>>=
options(prompt = "R> ", continue = "   ", width = 80)
L <- installed.packages()
ind <- !is.na(match(L[, "Package"], "SeaLev"))
Renext.Version <- L[ind, "Version"]
RVersion <- R.Version()$version.strin
@


\begin{abstract}
  
  The \verb@SeaLev@ package has been specified by IRSN.  The main goal
  is to implement the convolution-based method called \textit{Joint
  Probability Method} as used in extreme Sea Level Analysis.  The
  package allows approximate inference based on the ``delta method''.
\end{abstract}

\pagebreak
\pagenumbering{arabic}  % 1, 2, 3, 4, ...
\setcounter{page}{1}

\chapter{Introduction}
%%======================

\begin{Prov}
  This document is based on \textbf{SeaLev \Sexpr{Renext.Version}}  
  using \Sexpr{RVersion}. 
  This is a DRAFT version. The functions calls may change in future
  versions.
\end{Prov}


%% \section*{Acknowledgments}
%%---------------------------
%  We gratefully acknowledge the BEHRIG\footnote{IRSN \textit{Bureau
%     d'Expertise Hydrog\'eologique, Risques d'inondation et
%     g\'eotechnique}.} members for their major contribution to
% designing, documenting and testing programs or datasets: Claire-Marie
% Duluc, Lise Bardet, Laurent Guimier and Vincent Rebour. We also
% gratefully acknowledge Yann Richet who encouraged this project from
% its begining and provided assistance and many useful advices.

\section{Outlook}
%%-------------
\subsection{Goals}
%%----------------
\textbf{SeaLev} is an R package~\cite{RMANUAL} dedicated to the
probability analysis of high Sea Levels using the \textit{Convolution
  Method} or \textit{Joint Probability Method} (JPM) as described in
the original articles of Pugh and Vassie~\cite{PUGHVASSIE79} and \cite{PUGHVASSIE80}.
More information on the context can be found in the book of David
Pugh~\cite{PUGHBOOK1}, chap.~8, or that (in french) of Bernard
Simon~\cite{SIMON}, chap.~VIII. 

The method concerns a \textit{still water} sea level, and relies on a
decomposition of it as the sum of a \textit{tide} part, and a
\textit{non-tidal} part --~or \textit{surge} part. The two components
are considered as independent random variables, and the probability
distribution of the sea level can then be computed by
convolution. Note that the surge part can not be observed by itself
and is obtained as the difference between the observed level and 
its tidal prediction. The surge is sometimes called the \textit{tide residual}.


The independence assumption between the tide and the surge is best
supported when the modelled sea level is recorded at or near high
tide~\cite{COLESBAYES}. The \index{skew surge} \textit{skew surge} is
computed as minus the difference of the predicted (astronomical) tide
and the nearest experimented high water. Modelling high tide sea levels
and skew surges (rather than, say,  hourly levels and surges) is a valuable
option as far as the interest is on extreme \textit{high} sea
levels. The time between two successive high tides is about $12$ hours and $26$
minutes for semi-diurnal tides, corresponding to a sampling rate of
\index{rate, sampling for high tides}
about $705.8$ high tides by year. The method of convolution applied
with Skew Surges is sometimes called the \textit{Skew Surge Joint Probability
Method} (SSJPM).

\subsection{Limitations}
%---------------------
The hypotheses retained for the convolution method are quite strong.  Besides
the independence it is assumed that no long-term trend exist in the tide or in
the surge process, and therefore a possible change in climate or sea level can
not be taken into account.


\section{Context}
%%-----------------
\subsection{Notations}
%%---------------------
Let~$Z$ denote the sea level random variable, and 
let $X$ and $Y$ be the tide and the non-tidal or surge part
$$ 
   \underset{\mathsf{sea\:level}\rule{0pt}{8pt}}{Z}
   = \underset{\mathsf{tide}\rule{0pt}{8pt}}{X} 
   + \underset{\mathsf{surge}\rule{0pt}{8pt}}{Y}
$$
We will use the notation $f_Z(z)$, $F_Z(z)$ and $S_Z(z)=1-F_Z(z)$ to represent
density, distribution and survival functions of $Z$, and similar notations for
another random variable the symbol of which will appear as a subscript. Subscripting
with a random variable symbol will also be used for parameters as in $\mu_Y$ 
or $\sigma_Y$.

Recall that when $X$ and $Y$ are independent and of continuous type
with densities $f_X(x)$ and $f_Y(y)$, the random variable $Z$ has a
density given by the convolution formula 
\index{convolution!formula}
\begin{equation}
  \label{eq:CONVDENS}
   f_Z(z) = \int_{-\infty}^{+\infty}
   f_X(x) \,f_Y(z-x) \,\mathsf{d}x
\end{equation}
A similar relation can be given using the survival functions 
\begin{equation}
  \label{eq:CONVSURV}
   S_Z(z) = \int_{-\infty}^{+\infty}
   f_X(x) \,S_Y(z-x) \,\mathsf{d}x
 \end{equation}
In both integrals, the variable of integration could be chosen 
to be a surge $y$, replacing then $x$ by  $z-y$.

 
\subsection{Tidal $X$}
%%---------------------
The distribution of the tidal component $X$ is assumed to be of
continuous type with a bounded support 
$$
\Low{x} \leqslant X \leqslant \Up{x}
$$
Therefore the bounds of the integrals in~(\ref{eq:CONVDENS})
and~(\ref{eq:CONVSURV}) can be replaced by $\Low{x}$ and
$\Up{x}$. The density $f_X(x)$ is assumed to be available in
a general non-parametric form.  \index{non-parametric} It is assumed
in the current version of \textbf{SeaLev} that the density $f_X(x)$ is
continuous and takes the value $0$ at the end-points of $X$
\begin{equation}
   \label{eq:FX0}
   f_X(\Low{x}) = 0 \qquad f_X(\Up{x}) = 0
\end{equation}
This condition is used in the numerical convolution.

\begin{remarks}
\item In practice, $\Up{x}$ will be the \textit{Highest Astronomical
    Tide} (HAT) which necessarily occurs at high tide. The minimum
  $\Low{x}$ will be for high tides. 
  \index{HAT (Highest Astronomical Tide)}%
\item For semi-diurnal tides, the distribution $f_X(x)$ of astronomical high
  tides will often be bi-modal.
\item The conditions (\ref{eq:FX0}) are not always fulfilled by an arbitrary
  periodic oscillation with range $(\Low{x},\,\Up{x})$,
  see the example in~\cite{PUGHVASSIE80}, p.~975.
\end{remarks}

\subsection{Non-tidal  component  $Y$ (surge)}
%%--------------------------------------------
For the non-tidal component or surge component $Y$, the requirement
will be that of the distribution of $Y$ conditional on $ Y> u$, where
$u$ is the threshold.  Such a distribution typically results from a
\textit{Peak Over Threshold} (POT) modelling.  
\index{POT (Peak Over the Threshold)}
The distribution will often be given for the excesses $Y^\star =
Y-u$ rather than for~$Y$. It will be given as a specific element
within a list of supported distributions, among which we find the
Generalised Pareto used in traditional POT.
\index{GPD (Generalised Pareto Distribution)}

As in standard POT analysis, the distribution of the excess 
must come with a \textit{rate} related to an underlying Homogeneous Poisson
Process. We assume that \textbf{the rate $\lambda$ is
expressed as a number of threshold exceedances by year}.

A desirable mathematical property for the density of $Y$ is continuity.
For physical reasons, the density should be bounded near the
threshold. This should put offside Weibull or gamma distributions with
increasing hazards, that is with $0< \verb@shape@ <1$ in both cases. However
using such distributions is possible in \textbf{SeaLev}.

\textbf{SeaLev} contains a description of some "special" distributions
for POT.  See table~\ref{TABDIST}. It is also possible to use other
distributions for excesses or even non-POT and hence an unconditional
distribution for the surge. In this case, the distribution
is for the the variable $Y$ and not for any kind of
excess. See~\ref{NONPOT} page~\pageref{NONPOT} for an example
using the Generalised Extreme Values (GEV) distribution. GPD and
GEV distributions are provided in suitable form by the \textbf{evd}
package~\cite{EVDRNEWS}, available from the CRAN.
\index{GEV (Generalised Extreme Values)}

\begin{table}
  \centering
  \begin{tabular} {p{6cm} l l l}
    \hline
    \multicolumn{1}{c}{\textbf{distribution}} \rule{0pt}{1.1em} &
    \multicolumn{1}{c}{\textbf{code}} &
    \multicolumn{1}{c}{\textbf{par. names}} & 
     \multicolumn{1}{c}{\textbf{package}} \\    
    \hline
     exponential        & \verb@exp@      & \verb@rate@  & \rule{0pt}{1.1em}\\
     generalised Pareto & \verb@GPD@      & \verb@scale@, \verb@shape@ & \textbf{Renext} \rule{0pt}{1.1em}\\
                        & \verb@gpd@      & \verb@scale@, \verb@shape@ & \textbf{evd} \\
     gamma              & \verb@gamma@    & \verb@shape@, \verb@scale@ & \rule{0pt}{1.1em}\\
     Weibull            & \verb@weibull@  & \verb@shape@, \verb@scale@ & \rule{0pt}{1.1em}\\
     mixture of two exponentials & \verb@mixexp2@  &
     \verb@prob1@, \verb@rate1@, \verb@delta@ & \textbf{Renext}\rule{0pt}{1.1em} \\
     \hline
  \end{tabular}
  \caption{\label{TABDIST}Distributions for surge POT. Some distributions require 
    the use of a specific (CRAN) package.}
\end{table}  



\subsection{Probability versus theoretical frequency}
%%------------------------------------------------
The tidal part $X$ has a deterministic nature and is related to a
deterministic cyclic process~$X_t$~\cite{JPMREPORT}. Yet we may speak
of ``probability distribution'' for observations $X_t$ provided that
some points are well understood.

Let $X_t$ be the series of computed tidal sea levels at successive high tide
times~$t=1$, $2$, $\dots$. This is a cyclic deterministic process with
a fairly large period\footnote{The nodal cycle, about $18.61$ years}.
The density $f_X(x)$ is such that the probability that $X$ falls in some
given interval should be equal to the correspondent frequency on large
periods. This can be expressed as an \textit{ergodicity condition}:
\index{ergodicity} for any arbitrary ``test'' function $\phi(x)$
defined on the support $(\Low{x},\,\Up{x})$, the
approximation
\begin{equation}
  \label{eq:ERGOS}
  \frac{1}{T} \sum_{t=1}^T \phi(X_t) \approx 
  \int \phi(x)f_X(x) \, \mathsf{d}x 
\end{equation}
must hold for large $T$ (relative to the period).  The independence condition
between $X$ and $Y$ can similarly be expressed using cross-frequencies over a
large number of periods or equivalently using an ergodicity condition involving
an arbitrary two-variables test function $\phi(x,\,y)$
\begin{equation}
   \label{eq:ERGOSIND}
  \frac{1}{T} \sum_{t=1}^T \phi(X_t,\,Y_t) \approx 
  \iint \phi(x,\,y)f_X(x)f_Y(y)\, \mathsf{d}x \,\mathsf{d}y
\end{equation}
for large $T$.  

\section{Convolution and POT}
%%-------------------------
The independence condition (\ref{eq:ERGOSIND}) implies that the distribution of
$X$ conditional on~$Y>u$ is identical to the unconditional distribution of~$X$.
Hence if a POT analysis is used for~$Y$, we still can use the convolution
distribution for~$Z$ with some restrictions. Firstly, the return levels for $Z$
are computed by considering that $Z$ values are sampled at a rate
\index{rate, sampling for high tides}
$\lambda$ with $\lambda<705.8$. Secondly, since the distribution of $Z$ is then only partially known,
the formula~(\ref{eq:CONVSURV}) can only be used for $z > x_{\mathrm{max}} +
u$. Actually, since $X \leqslant \Up{x}$ with probability one, we have
for $z > \Up{x} + u$
$$
   S_Z(z) = \Pr(Z>z) = \Pr\pCond{Z > z}{Y>u} 
   %%\qquad \mathrm{for} \quad z > \Up{x} + u
$$
and the distribution of $Y$ conditional on $Y>u$ can 
be used in place of the unconditional one.

\begin{remarks} 
\item In the POT context, the needed independence between $X$ and $Y$ turns into
  a weaker condition of independence conditional on~$Y>u$. There could be some
  dependence between $X$ and $Y$, but this must be limited to small $Y$.
\item The GPD distribution of $Y$ can have a finite upper end-point
  (with negative shape parameter $\xi_Y<0$).
\end{remarks}




\section{Return periods and return levels}
%%-------------------------------------------
The rate $\lambda$ is used to compute the return period $T_Z(z)$ of a
given level~$z$ according to
$$
   T_Z(z) = \frac{1}{\lambda \times S_Z(z)}
$$
This formula will be used for $z > \Up{x} + u$. An approximated
value of $S_Z(z)$ will be computed with the convolution formula.

In most cases, the distribution of $Y$ is estimated within a parametric
family. In the POT context, the rate $\lambda$ will be replaced by an estimation
$\widehat{\lambda}$. When instead all high tide measurements are used, the rate
must be considered as certain with value $\lambda = 705.8\,\mathrm{year}^{-1}$.

Note that when $Y$ has a finite upper end-point $y_{\mathsf{max}}$, the sea level
$Z$ also has finite upper end-point $z_{\mathsf{max}}$. This finite level
corresponds to an infinite return period $ T_Z(z_{\mathsf{max}})=+\infty$.

We may alternatively be concerned with the return level
$z(T)$ corresponding to a given return period, e.g. $T=1000\,\mathrm{years}$.
This level is obtained as the solution~$z$ of
\begin{equation}
   \label{eq:RETLEV}
   S_Z(z) = \frac{1}{\lambda T}
\end{equation}
The return level $z(T)$ can be expressed as
\begin{equation}
   \label{eq:RETLEV2}
   z(T) = q_Z(p), \qquad p := 1-\frac{1}{\lambda T}
\end{equation}
where $q_Z(p)$ is the standard quantile function defined for $0<p<1$.


\section{Inference based on the delta-method}
%%-----------------------------------
\subsection{Principle}
%%------------------
\index{delta method}
The distribution of the tidal part $X$ is assumed to be perfectly known.
The (conditional) distribution of $Y$ depends on a parameter $\bs{\theta}_Y$
of length $p_Y$. The parameter vector is in the general case
$$
   \bs{\theta} = [\lambda,\,\bs{\theta}_Y^\top ]^\top
$$
The threshold $u$ for $Y$ is considered as fixed.  For instance, when
the GPD is used for $Y$ in a POT analysis, the two estimated
parameters are the scale and shape $\bs{\theta}_Y = \left[
  \sigma_Y,\,\xi_Y \right]^\top$, while the location parameter $\mu_Y$
coincides with the threshold $u$, hence is fixed.

The \textit{delta method} is a general framework for approximated
inference, see~\cite{COLESBOOK}. It can be used in the convolution
context, where the uncertainty on the distribution of~$Y$ propagates
on the distribution of $Z$. The survival $S_Z(z)$ depends on
$\bs{\theta}_Y$ according to
$$
   S_Z(z;\bs{\theta}_Y) = \int_{\Low{x}}^{\Up{x}} 
   f_X(x) \,S_Y(z-x;\bs{\theta}_Y) \,\mathsf{d}x
$$
Under some mild assumptions, the derivative of the survival $
S_Z(z;\bs{\theta}_Y)$ with respect to the parameter~$\bs{\theta}_Y$
can be obtained by differentiating under the integral sign
$$
\frac{\partial}{\partial \bs{\theta}_Y} \, S_Z(z;\bs{\theta}_Y) =
\int_{\Low{x}}^{\Up{x}} f_X(x) \,
\frac{\partial}{\partial \bs{\theta}_Y} S_Y(z-x;\bs{\theta}_Y)
\,\mathsf{d}x
$$
The partial derivative in the integral is computed numerically using a finite
difference. The partial derivative for $S_Z(z)$ is then obtained by convolution.

\subsection{Return levels}
%%----------------------
For a fixed period $T$, the corresponding return level $z$ depends on
$\bs{\theta}_Y$ and $\lambda$, and therefore should be noted
$z(T;\,\bs{\theta})$. Indeed in the equation (\ref{eq:RETLEV}) the left hand
should actually be written $S_Z(z;\,\bs{\theta}_Y)$ in place of $S_Z(z)$. The
partial derivatives of $z$ with respect to $\bs{\theta}_Y$ and $\lambda$ can be
obtained through the derivation of an implicit function.
%  \begin{Prov}
%    A mettre en annexe
%  \end{Prov}
%  For $\bs{\theta}_Y$
%  $$
%  \frac{\partial S_Z(z;\,\bs{\theta}_Y)}{\partial z} \times
%  \frac{\partial z}{\partial \bs{\theta}_Y} + 
%  \frac{\partial S_Z(z;\,\bs{\theta}_Y)}{\partial \bs{\theta}_Y} = \m{0}
%  $$
Using the fact that $\partial S_Z/\partial z$ is the opposite of the density
$f_Z$, we get
 \begin{equation}
    \label{eq:DERT}
 \frac{\partial z}{\partial \bs{\theta}_Y}= 
 \frac{1}{f_Z(z)} \times \frac{\partial S_Z(z)}{\partial \bs{\theta}_Y},%
 \qquad 
 \frac{\partial z}{\partial \lambda}= 
 \frac{1}{\lambda^2 T f_Z(z)}
\end{equation}
where the dependence on $\bs{\theta}_Y$ has been omitted in the density $f_Z(z)$
and survival $S_Z(z)$.


\section{Bayesian inference (Monte-Carlo)}
%%-----------------------------------------
In a Bayesian framework, one may have a posterior distribution for
$\bs{\theta}$, say $p\pCond{\bs{\theta}}{\m{Y}}$. A popular form for
posterior distribution is a discrete approximation as a mixture 
of Dirac masses at $K$ outcomes
$$
   \bs{\theta}^{[1]}, \: \bs{\theta}^{[2]}, \,
   \: \dots, \: \bs{\theta}^{[K]}
$$
Such a distribution can be provided as a matrix with columns in
correspondence with the parameters.  Each row of the matrix contain a
random drawing $\bs{\theta}^{[k]}$ from the posterior of $\bs{\theta}$.
The random drawings are generally not independent Markov Chain Monte
Carlo (MCMC).  The posterior distribution of a return level
$T_Z(z;\,\bs{\theta})$ for a fixed level~$z$ has a straightforward 
discrete approximation. The posterior mean of the return period is estimated
by the mean value
$$
   \Esp\bCond{T_Z(z)}{\m{Y}} \approx
    \frac{1}{K}\,\sum_{k=1}^{K} \frac{1}{\lambda^{[k]} \times S_Z(z;\,\bs{\theta}_Y^{[k]})}
$$
and a similar formula will work for posterior moments or quantiles.

\begin{Prov}
The Bayesian inference is not implemented yet.
\end{Prov}

\section{Expectation of the tide conditional on the Sea Level}
%%-----------------------------------
\label{TIDECONDSL}
The importance of the tide in the formation of extreme sea level
combinations can be investigated using the conditional expectation of
the tide $X$ given the sea level $Z$, that is
\begin{equation}
  \label{eq:ESPXCONDZ}
  \Esp\pCond{X}{Z=z} = \frac{\int x \,f_X(x)f_Y(z-x) \,\mathsf{d}x}%
    {\int f_X(x)f_Y(z-x) \,\mathsf{d}x} =: g(z).
\end{equation}
The expectation provides an ``inverse'' prediction: for a given high
sea level~$z$, what tide $x$ should be expected on average? Note that conditional
on~$Z=z$ the distribution of~$X$ will not in general be unimodal, and
several scenarios of tide can occur. 

The two integrals in the fraction of~(\ref{eq:ESPXCONDZ}) are on the interval
$(\Low{x},\,\Up{x})$ and can be computed
numerically using a discrete convolution. 

The behaviour of the function $g(z)$ for large $z$ mainly depends on
the distribution of $Y$ and of some global features of the 
distribution of~$X$. It can be shown that when
$Y$ follows an exponential distribution $g(z)$ is constant for large $z$.  
Surprisingly enough, some
distributions of $Y$ lead to a function $g(z)$ which is decreasing for
$z$ large enough. Thus if $Y$ is
$\verb@GPD@(\mu_Y,\,\sigma_Y,\,\xi_Y)$ with $\xi_Y > 0$, it can be
shown that $g(z)$ is decreasing for $z \geqslant \Up{x} +
\mu_Y$ and tends to the unconditional expectation $\Esp(X)$ when $z$
tends to~$+\infty$. This fact can be related to the asymptotic behaviour
of the distribution of~$Z$.


\section{Return level plot}
%%-----------------------------
\index{return level plot}
The return level plot is a general tool in extreme values analysis. It
is often used to compare a fitted distribution for extreme values
(e.g. POT) with experimental points. Usually the points are located at
the largest order statistics of a sample.

The return level plot of \textbf{SeaLev} plots a distribution as a
curve with points $[T,\,z(T)]$ where $z(T)$ is obtained
by~(\ref{eq:RETLEV2}). It uses a logarithmic scale for periods, and an
ordinary scale for levels, thus the points actually plotted are couples
$[\log(T),\,z(T)]$ where $T = [\lambda \times(1-p)]^{-1}$ and $z(T)=
q_Z(p)$. When the distribution of $Z$ is close to the exponential, the
theoretical curve is nearly a straight line. This will also be true for
large return periods if the distribution of $Z$ falls in the Gumbel
domain of attraction.

In the present context, the interest is focused on the sea level~$Z$.
Since the distribution of $Z$ results from the convolution and not from an
estimation, there will generally be no use of experimental
values for $Z$. In the POT context, the computation is exact only
for $z > \Up{x} + u$, and thus \textbf{only the corresponding part of the
return level curve must be used then}.

When experimental points are to be added to the plot, the two 
\index{experimental points}
formals \verb@z@ and \verb@duration@ are needed in the functions 
to compute the plotting
positions.  \index{plotting positions} In the simplest case, \verb@z@ is a
numeric vector and \verb@duration@ is a positive numeric value representing a
duration in years. The values in \verb@z@ are assumed to be the $r$-largest
values $Z_k$ of the sea level during a period having the specified duration. The
underlying total number of seal levels $n_{\textsf{H}}$ is the number of high
tides corresponding to the yearly rate $\lambda_{\textsf{H}}
=705.8\,{\textrm{year}}^{-1}$. Assuming that the $Z_k$ are in decreasing order, the
return period $\widetilde{T}_k$ used for $Z_k$ is such that
$1/(\lambda_{\textsf{H}}\,\widetilde{T}_k)$ is the estimated probability of
exceedance of $Z_k$, i.e.
$$
    \frac{1}{\lambda_{\textsf{H}}\,\widetilde{T}_k} = \frac{k}{n_{\textsf{H}}+1} = 
    \frac{k}{\lambda_{\textsf{H}} w +1} 
$$
where $w$ is the duration in years. For instance if $w=10\,\textrm{years}$,
the largest experimental level $Z_1$ is considered as the largest 
value among $n_{\textsf{H}}=705.8 \times 10 = 7058$ levels, corresponding
to a probability of exceedance of $1/7059$, and to a return period 
of $7059/705.8 \approx 10$~years.
The rationale of the formula is
that the tides $X_k$ corresponding to the $Z_k$ are assumed to occur
at the same rate as randomly chosen tides. It is possible to specify 
several vectors using a list for \verb@z@, the duration being then a vector
or a list with the same length as \verb@z@. See section~\ref{EMPPOINTS} 
page~\pageref{EMPPOINTS} for an example.


\section{Special case: exponential surges }
%%------------------------------------------------------
A special case of interest is when $Y$ has an exponential distribution with
location $\mu_Y$ and scale $\sigma_Y$.
It turns out then that the distribution of $Z$ conditional on $Z > \Up{x} + \mu_Y$ 
is also exponential. More precisely for $z > \Up{x} + u$ the value
of the survival $S_Z(z)$ is identical to $S_{z^\star}(z)$ with $Z^\star := \mu_X^\star + Y$ and
\begin{equation}
  \label{eq:muXStar}
  \mu_X^\star := \sigma_Y \log \Esp\left[ e^{X / \sigma_Y} \right] = \sigma_Y K_X(1 / \sigma_Y).
\end{equation}
where $K_X(t) := \log \Esp[e^{tX}]$ is the generating function of the
cumulants of~$X$. We also have $\Esp\pCond{X}{Z=z} = \mu_X^\star$ for
$Z^\star := \mu_X^\star + Y$.  In other words, the return levels of
$Z$ are identical to those that would be obtained with a constant
astronomical tide $X \equiv \mu_X^\star$.  Note that $\Esp[X] \leqslant \mu_X^\star
\leqslant  \Up{x}$ so the expected tide corresponding to large sea 
levels~$Z$ falls somewhere between the unconditional mean tide and the maximal tide.


When $Y \sim \texttt{GPD}(\mu_Y,\,\sigma_Y,\,\xi_Y)$ and the shape 
is small $\xi_Y \approx 0$, the distribution of $Z$ can be approximated as 
$\texttt{GPD}(\mu_X^\star + \mu_Y,\,\sigma_Y,\,\xi_Y)$ with $\mu_X^\star$ as above 
in~(\ref{eq:muXStar}).

\begin{remark}
  A comparable approximation is used in~\cite{ColesCoastalFlood}
  for annual maxima of sea level considered as GEV. The impact of the tide on the distribution 
  of annual maxima is a shift which is the mean value of $\sigma \exp(X_t/\sigma)$
  where $\sigma$ is the GEV scale parameter which plays the same role as the GPD scale
  for the tail. In view of (\ref{eq:ERGOS}) for above for $\phi := \exp$, the two shifts can be compared.
  \index{GEV (Generalised Extreme Values)}
\end{remark}











\chapter{Using the \code{convSL} function}
%%==============================================
\section{Goals}
%%--------------------------------------
The \verb@convSL@ function computes return levels for the sea level $Z$ using
the distributions for $X$ and $Y$ given on input.  It returns several objects
among which a ``prediction'' table associating return periods or probabilities
to return levels. When possible, approximate confidence limits are computed using
the delta method. 

This function is not concerned with estimation tasks (e.g. POT), which should
rely on other packages.


\section{Non-parametric density of $X$}
%%--------------------------------------
The density of $X$ must be a list with elements \code{x} and \code{y}. It can be
an R object of the (S3) class \verb@density@, such as computed with the
\verb@density@ function of the \pkg{stats} package. The range of $X$ is
determinated as the range of the \verb@x@ elements.

The dataset \verb@Brest.tide@ from \verb@SeaLev@ provides
an example of estimated density for high-tide sea levels in Brest.
<<label=BrestTide, fig=TRUE, include=FALSE>>=
library(SeaLev)
convSL <- convSL2
data(Brest.tide)
class(Brest.tide)
str(Brest.tide)
plot(Brest.tide, col = "SeaGreen", type = "l",
     main = "Density of high-tide sea level in Brest")
grid(); abline(h = 0)
@ 
The \verb@plot@ function call and subsequent graphics calls produce the
plot on the left of the figure~\ref{BrestTide}.

The level $X$ is given in centimetres, the density values are
accordingly in $\textrm{cm}^{-1}$.  As implicitly admitted when
plotting densities, it will be assumed that linear interpolation can
be used to evaluate $f_X(x)$ on another grid of values, usually a
finer one. The required normalisation condition is that the
trapezoidal rule for numerical integration should lead to an integral
equal to~$1.0$. Provided that the density values are zero at
end-points, the rectangles rule should also give the same value~$1.0$.
<<label=BrestTide0>>=
c(Brest.tide$y[1], Brest.tide$y[length(Brest.tide$y)])   ## values at end-points
h <- diff(Brest.tide$x)[1]                               ## grid step
h*sum(Brest.tide$y)                                      ## check integral
@ 
This checks could be replaced in future versions by the registration
of a formal class for discredited densities.

\section{Convolution}
%%======================
\subsection{Specifying parameters for $Y$}
%----------------------------
The parameter values (generally estimated) must be given as a named
list or a numeric vector with named elements. For instance, consider 
high-tide skew surges for Brest, and assume that in a POT analysis using
a threshold $u=50$ cm we got the estimated parameters $\sigma_Y = 10$~cm
(scale) $\xi_Y = -0.01$ (shape), and that the exceedances occurred
at a rate of \verb@1.6@~$\mathrm{years}^{-1}$. 
\index{rate, sampling for high tides}
\index{skew surge}
We can store these informations
as R objects say \verb@u@, \verb@theta.y@ and \verb@lambda@
<<label=BrestSurgePar1>>=
u <- 50 
theta.y <- list(scale = 10, shape = -0.01)
lambda <- 1.6
@ 
Note that we can use a numeric vector created with the
\verb@c@ function rather than a \verb@list@, but in both cases 
\verb@names@ must fit the parameters of the distribution
<<label=BrestSurgePar2>>=
theta.y <- c(scale = 10, shape = -0.01)
names(theta.y)
@ 
Now we can use the created objects in the arguments \verb@threshold.y@,
\verb@par.y@ and \verb@lambda@ of the convolution function.
<<label=BrestConv0, fig=TRUE, include=FALSE>>=
conv.gpd0 <- convSL(dens.x = Brest.tide,
                   threshold.y = u, distname.y = "gpd",
                   lambda = lambda, par.y = theta.y,
                   main = "Sea-level with GPD surges: given parameters")
@
\index{return level plot}
By default, a return level plot is produced as in figure~\ref{BrestTide}. No ``confidence
band'' can be plotted since no information was given about 
estimation uncertainty. 

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-BrestTide.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConv0.pdf} 
     %%\includegraphics[width=7cm]{Rgraphics/fig-RenplotGaronne.pdf} 
   \end{tabular}
   \caption{\label{BrestTide}Left panel:  density of the high-tide 
   sea level $X$ in Brest (France). Right panel: return level plot 
   using convolution and a known GPD distribution for the surge $Y$. 
   Only the part above the horizontal red line should be used, 
   corresponding to return periods over about $200$~years. 
 }
\end{figure}

\subsection{Using a fitted POT model}
%%---------------------------------
\index{POT (Peak Over the Threshold)}
The estimated values for the surge can be computed using
\textbf{Renext} and its \verb@Brest@ dataset.  The arguments to be
passed to the \verb@Renouv@ function then include the vector of surges
\verb@x@ and the effective duration (in years) in order to estimate
the rate \verb@lambda@ (in inverse years).
%%-----------------------------------------------------
<<label=BrestFitSurge, fig=TRUE, include=FALSE>>=
library(Renext); data(Brest) 
fit.gpd1 <- Renouv(x = Brest$OTdata$Surge,
                   effDuration = as.numeric(Brest$OTinfo$effDuration),
                   threshold = 50, distname.y = "gpd",
                   main = "GPD surge")
coef(fit.gpd1)
@ 
%%-----------------------------------------------------
The estimated parameters are very close to those used before.  The fit
produces the return level plot shown on the left of~\ref{BrestSurge}, with a
$100$-years return level of about $100$~cm. The fitted object contains a
covariance matrix of estimation.
<<label=BrestSL>>=
cov1 <- vcov(fit.gpd1)
cov1
@ 
This matrix can be used in the \verb@covpar.y@ formal argument of
\verb@convSL@ function. As it is the case here, the matrix must have
rownames and colnames, and these must agree with the parameter names
of the distribution.

<<label=BrestConv1, fig=TRUE, include=FALSE>>=
conv.gpd1 <- convSL(dens.x = Brest.tide,
                    threshold.y = 50,
                    distname.y = "gpd",
                    lambda = lambda, par.y = theta.y,
                    covpar.y = cov1,
                    main = "Sea-level for Brest with GPD surges")
@ 
We get the return level at the right of figure~\ref{BrestSurge}, in
which (pointwise) confidence bands are drawn for the return levels.
These are obtained by ``propagating the uncertainty'' on the parameters (as
quantified by the covariance) to the return levels~$z(T)$. This is
done using the delta method and the partial
derivatives~(\ref{eq:DERT}).
\index{delta method}

The plot can be enhanced by filling the confidence region(s) and
using colours. The confidence levels can be set using \verb@pct.conf@.
\index{confidence bands!filled}
\index{confidence bands!percentage}
<<label=BrestConv15, fig=TRUE, include=FALSE>>=
conv.gpd2a <- convSL(dens.x = Brest.tide,
                     threshold.y = 50,
                     distname.y = "gpd",
                     lambda = lambda, par.y = theta.y,
                     pct.conf = c(95, 90),
                     filled.conf = TRUE, mono = FALSE,
                     covpar.y = cov1,
                     main = "Sea-level for Brest with GPD surges (lambda known)")
@ 
The plot is shown on the left panel of figure~\ref{BrestSurge2}.

It is possible to use in \verb@convSL@ a covariance matrix without
\verb@lambda@. For instance, dropping the first row and the first column
in \verb@cov1@
<<label=BrestSL>>=
cov1[-1, -1]
@ 
leads to a matrix that can be used with \verb@convSL@. The same
effect can be obtained by specifying a \verb@use.covlambda@ argument
with \verb@FALSE@ as its value.
<<label=BrestConv2, fig=TRUE, include=FALSE>>=
conv.gpd2 <- convSL(dens.x = Brest.tide,
                    threshold.y = 50,
                    distname.y = "gpd",
                    lambda = lambda, par.y = theta.y,
                    use.covlambda = FALSE,
                    pct.conf = c(95, 90),
                    filled.conf = TRUE, mono = FALSE,
                    covpar.y = cov1,
                    main = "Sea-level for Brest with GPD surges (lambda known)")
@ 
The plot is shown on the right panel of
figure~\ref{BrestSurge2}. The effect of ignoring the uncertainty on
\verb@lambda@ is to produce a narrower confidence band for small
return periods. The effect for large periods is negligible.

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-BrestFitSurge.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConv1.pdf}  
   \end{tabular}
   \caption{\label{BrestSurge} Fitting a POT model for Brest surge with \textbf{Renext} (left),
     and using the fitted distribution within a convolution.
   }
\end{figure}

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     %%\includegraphics[width=7cm]{Rgraphics/fig-BrestFitSurge.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConv15.pdf} 
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConv2.pdf}  
   \end{tabular}
   \caption{\label{BrestSurge2} Comparison of two convolutions of the
     tide with the fitted GPD.  On the left panel, the covariance
     concerns \texttt{lambda}. Right panel \texttt{use.covlambda =
       FALSE}. }
\end{figure}

\subsection{Using a fitted non-POT model}
%%---------------------------------
\label{NONPOT}
Although a POT model will be used in most cases, it is yet possible to
use a non-POT model, i.e.  a non-conditional distribution for~$Y$.
For illustration purpose only, assume that the surge at Brest can be
described by a Gumbel \index{Gumbel distribution} distribution with
parameters $\mu_Y=-10.8$~cm (location) and $\sigma_Y= 10$~cm (scale).  The
Gumbel assumption for surges is very close to that of exponentially
distributed excesses over a high enough threshold. Here the
parameters were chosen in accordance with the POT estimation: $\sigma_Y$
takes the same values as in the GPD case, while $\mu_Y$ was chosen to
give the same rate of exceedance over $u = 50$~\textrm{cm}.  
\index{GEV (Generalised Extreme Values)}

The arguments provided to \verb@convSL@ will be quite different than
in the GPD case.  We specify a non-POT distribution by using a
\verb@threshold.y@ with value \verb@NA@, and the rate \verb@lambda@
must now be $705.8\,\textrm{years}^{-1}$.

<<label=BrestConvGEV, fig=TRUE, include=FALSE>>=
par.y <- c(loc = -10.8, scale = 10)
res.gumbel <- convSL(dens.x = Brest.tide,
                     threshold.y = NA,           
                     distname.y = "gumbel",
                     lambda = 705.8,
                     par.y = par.y,
                     filled.conf = TRUE, mono = FALSE,
                     main = "Gumbel surges")
@

\noindent
The return level is shown on figure~\ref{BrestSurgeGEV}.  Note that
\verb@threshold.y@ is equal to its default value \verb@NA@ and that we
could have left \verb@lambda@ to its default value since this is
$705.8$ when \verb@lambda@ is \verb@NA@ (see the package manual). Thus
these two arguments could have been omitted in the call.

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConvGEV.pdf} 
     %%\includegraphics[width=7cm]{Rgraphics/fig-BrestConv2.pdf}  
   \end{tabular}
   \caption{\label{BrestSurgeGEV} Using a Gumbel distribution (non-POT)
     for the surge. }
\end{figure}


\section{Predictions}
%%---------------------
The computed return levels and confidence limits are returned within a
data.frame \verb@pred@. Here are the first rows.

<<label=BrestConv25>>=
head(conv.gpd2$pred, n = 3)
@ 

\noindent
Each row correspond to a given period $T$, e.g. $T=100$ years, and
give the corresponding probability of non-exceedance $p(T)$ (column
\verb@prob@), the corresponding return level $z(T)$ (column
\verb@quant@) as well as confidence limits for $z(T)$, here $70$~pct
and $95$~pct.  It is possible to specify the desired periods or
probabilities by using the \verb@pred.period@ and \verb@pred.prob@
formals of \verb@Renouv@.


Recall that $z(T)$ and $p(T)$ are connected to each other by $z =
1/[\widehat{\lambda} \times (1-p)]$, thus the relation between $T$ and
$p$ is not exactly known and is affected by the uncertainty on the
estimation of $\lambda$. However, this uncertainty is small for large
periods.

\index{prediction}
Also note that the term ``prediction'' can be misleading. The $100$-years
return level is the level that is exceeded on average once every
$100$~years. This level might occur twice or more in a given century.

When a POT model is used for the surges~$Y$, only periods corresponding
to levels $z > u + x_{\mathrm{max}}$ must be used, where $u$ is the threshold.

\section{Adjusting the return level plot}
%%-------------------------------------
\index{axes, controlling the range}
The axis limits can be adjusted using the \verb@ylim@ parameters and
the ``dots'' mechanism just like as for the \verb@main@ formal.  It
will generally be necessary to modify \verb@ylim@ in order to see the
conditional expectation curve~$\Esp\pCond{X}{Z=z}$ an in
<<label=BrestConv3, fig=TRUE, include=FALSE>>=
conv.gpd3 <- convSL(dens.x = Brest.tide,
                    threshold.y = 50, distname.y = "gpd",
                    lambda = lambda, par.y = theta.y, covpar.y = cov1,
                    ylim = c(300, 600),
                    main = "Sea-level for Brest with GPD surges (lambda known)")
@ 
leading to the plot on left of figure~\ref{BrestSurge3}. 

For the x-axis, which is in log-scale, it is preferable to 
work \verb@Tlim@ or \verb@problim@. In the first case, two limits
are given in time (hence years). In the second case they are given 
in probability (of non-exceedance). 
<<label=BrestConv4, fig=TRUE, include=FALSE>>=
conv.gpd3 <- convSL(dens.x = Brest.tide,
                    threshold.y = 50, distname.y = "gpd",
                    lambda = lambda, par.y = theta.y, covpar.y = cov1,
                    Tlim = c(100, 3000),
                    main = "Sea-level for Brest with GPD surges (lambda known)")
@ 
Note that the results are recomputed but the parameters \verb@Tlim@,
\verb@problim@ are purely graphical ones and have no impact on the
computed results.

The plot can be annotated with the standard functions from
the \textbf{graphics} package: \verb@text@, \verb@lines@, etc.
%%<<label=BrestConv4, fig=TRUE, include=FALSE>>=
%%text(x = mean(par()$usr[1:2]), y = 480, 
%%     cex = 2, "essai", col = "red")
%%@ 
Since the x-axis is in log-scale it will be simpler to use 
\verb@par()usr@ to get the world coordinates or 
to use the \verb@locator@ function.

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConv3.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-BrestConv4.pdf} 
   \end{tabular}
   \caption{\label{BrestSurge3} Changing the axes. Left: the
     \texttt{ylim} argument of \texttt{plot} was used. Right: the
     \texttt{Tlim} argument of \texttt{RSLplot} chooses the range of
     return periods, here from $100$ to $5000$.}
\end{figure}


In order to add experimental points to the plot, the arguments
\index{experimental points}
\verb@z@ and \verb@duration@ must be passed to the \verb@RSLplot@
\label{EMPPOINTS}. For instance, with meaningless points and in 
a purely illustrative purpose we get the plots of figure~\ref{empirical}.

<<empirical, fig=TRUE, include=FALSE>>=
res.g2 <- convSL(dens.x = Brest.tide,
                 threshold.y = NA, distname.y = "gumbel",
                 lambda = 705.8, par.y = par.y,
                 filled.conf = TRUE, mono = FALSE,
                 main = "Artifical empirical points (1 set)",
                 z = c(500, 490, 480, 460),
                 duration = 200)
@
<<empirical1, fig=TRUE, include=FALSE>>=
res.g3 <- convSL(dens.x = Brest.tide,
                 threshold.y = NA, distname.y = "gumbel",
                 lambda = 705.8, par.y = par.y,
                 filled.conf = TRUE, mono = FALSE,
                 main = "Artificial empirical points (2 sets)",
                 z = list(c(500, 490, 480), c(440, 420, 380, 350)),
                 duration = c(200, 170))
@  

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-empirical.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-empirical1.pdf} 
   \end{tabular}
   \caption{\label{empirical} Adding empirical points to the return level
     plot using \texttt{z} and \texttt{duration}. When several sets are used,
     \texttt{z} must be a list of numeric vectors.}
\end{figure}








\chapter{Frequently Asked Questions}
%%==============================================
\section{Calling \texttt{convSL}}
%%-------------------------------
\noindent
\textbf{Q.} Calling \verb@fRenouv@, I get an error with a message 
concerning a function \verb@qfun.y@.
\par\medskip\noindent
\textbf{A.} A plausible explanation is that the distribution for the
surge does not belong to the list of special distributions and that it
does not meet the requirements about functions names. It the second
case, it may be possible to redefine the probability functions by
using a ``wrapper''. For instance, if the distribution depends on a
parameter \verb@bar@ and has density \verb@foodens@, a new density is
defined 
<<label=FAQ1>>=
dMydist <- function(x, bar) foodens(x, bar = bar)
@ 
In order to use \verb@"Mydist"@ as a possible \verb@distname.y@ choice, 
the same thing must be done for the
distribution and the quantile functions which must have name
\verb@pMydist@ and \verb@qMydist@.

%%---------------------------------------------------------
\par\medskip\noindent
\textbf{Q.} I do not want to use a threshold $u$ for $Y$ nor to describe
excesses, but rather to use a known distribution for~$Y$.
\par\medskip\noindent
\textbf{A.}
Make sure that the distribution meet the requirements on probability 
functions (see question above), and proceed as in the example
of \ref{NONPOT} page~\pageref{NONPOT}.
% This situation morally corresponds to $u=-\infty$. However $Y-u$ is
% then meaningless.  A possible solution is to take a very small value
% for $u$ (say $u=-100$ if $Y$ is in meters) and to unshift the
% distribution of $Y$ using the \verb@shift.y@
%%--------------------------------------------------------
\par\medskip\noindent
\textbf{Q.} I have a warning message when calling \verb@convSL@ mentioning that
something \verb@"is not a graphical parameter"@.
\par\medskip\noindent
\textbf{A.}  Due to the \verb@dots@ mechanism, no check is possible
for the formal arguments of \verb@convSL@. When a formal argument is not
found in the list of arguments for \verb@convSL@ it is passed to
\verb@RSLplot@ and possibly then to \verb@plot@.  When a formal is
given which does not belong to any of the three argument lists, a
message is addressed containing the above mentioned words. The formal will not be taken
into consideration Most likely, it is a misuse, and \textbf{the message 
must be read carefully}.



\section{Inference}
%%-------------------------------
\noindent
\textbf{Q.} The confidence intervals for return levels are of decreasing width 
when the level $z$ increases, which seems unnatural.
\par\medskip\noindent
\textbf{A.} Such a phenomenon can occur when the uncertainty about parameters is dominated
by the uncertainty on the rate $\lambda$. In the estimation variance of a return
level~$z(T)$, the part that can be attributed to $\lambda$ is computed using the
second partial derivative of (\ref{eq:DERT}) for  $\lambda =\widehat{\lambda}$. This gives
$$
    \Var( \widehat{\lambda} ) \times [ \partial z /\partial \lambda ]^2  =
   \Var( \widehat{\lambda} ) \times \widehat{\lambda}^{-4} T^{-2} f_Z(z)^{-2} 
$$
It might be the case that $T^{-2} f_Z(z)^{-2}$ is decreasing with~$T$. 
Note however that the uncertainty on the parameters of $Y$ is
usually the main source of uncertainty on the return levels for large
periods. Decreasing widths for the confidence intervals can also
result from a misuse of a fitted model: wrong units, error in
parameter names or order, etc.
%%--------------------------------------------------------
\section{Numerical precision}
%%-------------------------------
\noindent
\textbf{Q.} What about the numerical error? What is the maximal
period that can be trustfully be used?
\par\medskip\noindent
\textbf{A.} The numerical precision depends on the distributions used
for $X$ and $Y$, and no general indication can be given at the
time. As an order of magnitude, the use of probability of exceedances about
$10^{-5}$ seems viable for distributions of surges with exponential excesses
(i.e. within the Gumbel domain of attraction). 


\appendix

\chapter{Numerical computation}
%%----------------------------------
\section{Discrete  convolution}
%%--------------------------------
In this section we will consider vectors with $0$ as starting index,
e.g. a vector $\m{a}$ of length $N$ writes 
$$
   \m{a} = [ a_0,\,a_1,\,\dots,\,a_{N-1}]^\top
$$
Such a vector can be related to the polynomial
$$
  a(\lambda) = a_0 + a_1\,\lambda + a_1\,\lambda^2 + \dots 
  + a_{N-1} \,\lambda^{N-1}
$$
Using two vectors $\m{a}$ and $\m{b}$ of length $N$ we can
compute their convolution product which is the vector~$\m{c}=[c_n]_n$
\begin{equation}
  \label{eq:CONV}
  c_n  = \sum_{k=0}^n a_{k}\,b_{n-k} 
  = \sum_{k,\,\ell \geqslant 0, \:k+\ell=n} a_{k}\,b_{\ell} 
\end{equation}
Note that $c_n$ is coefficient of $\lambda^n$ in the product of the two polynomials
related to $\m{a}$ and $\m{b}$ i.e.  $c(\lambda)= a(\lambda)\,b(\lambda)$. On a
plane grid of points with integer coordinates $(k,\,\ell)$, the coefficient is
obtained by summing products $a_kb_\ell$ on the line with equation $k+\ell =n$
with slope $-1$ (see figure \ref{FIGCONV}).

The product $c_n$ can be computed for any index $n\geqslant 0$ using the convention
that $\m{a}$ and  $\m{b}$ are completed by zeros e.g. $a_k=0$ for $k \geqslant N$.
Then $c_n$ can differ from zero for $n$ between $0$ and $2N-2$. Taking
$n=2N-2$ the sum (\ref{eq:CONV}) reduces to one summand
for $k=N-1$, namely $a_{N-1}\,b_{N-1}$ (see figure \ref{FIGCONV}). In other words
the convolution of two vectors of length $N$ has length $2N-1$.

It can be remarked that 
$$
    \sum_{n} c_n = \left[\sum_{k} a_k \right]
    \left[\sum_{\ell} b_\ell \right]
$$
which is easily checked using  $c(\lambda) = a(\lambda)b(\lambda)$ for $\lambda=1$.

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=5cm]{images/Convol.pdf} &
     %%\includegraphics[width=7cm]{Rgraphics/fig-RenplotGaronne.pdf} 
   \end{tabular}
   \caption{\label{FIGCONV} Convolution of two vectors of length $N=5$.
    If each point $[k,\,\ell]$ shows the product  $a_k\,b_\ell$, 
    then $c_n$ comes by summation over a segment of the line $k+\ell =n$. 
    For $n=2N-2$ (here $n=8$), the sum boils down to $k=\ell=N-1$.}
\end{figure}

\section{Continuous convolution}
%%---------------------------------
\subsection{Grids}
%-----------------
\index{convolution!discrete}
Consider the convolution integral of~(\ref{eq:CONVDENS})
\begin{equation}
   \label{eq:CONVDENS2}
 f_Z(z)
   = \int_{x_{\textrm{min}}}^{x_{\textrm{max}}} f_X(x)\,f_Y(z-x)\,\textrm{d} x
\end{equation}
The densities $f_X(x)$ and $f_Y(y)$ will be used in relation with 
two discrete regular grids $x_k$ and $y_k$. The two grids are assumed 
to have the same step~$h$ and the same number $N$ of intervals. For the $x$-grid, 
the intervals are $(x_k,\,x_{k+1})$ with
$$
    x_0 < x_1 < \dots < x_{N}  \qquad x_k = x_0 + k\times h \quad  (k \textsf{ integer})
$$
and similarly let $y_\ell= y_0+\ell \times h$ for integer~$\ell$. The grid
$x_k$ is assumed to cover the support of $f_X(x)$, i.e. $x_0 \leqslant
x_{\textrm{min}}$ and $x_{\textrm{max}} \leqslant x_N$.  Then from (\ref{eq:CONVDENS2})
\begin{equation}
  \label{eq:CONV0N}
  f_Z(z)
   = \int_{x_{\textrm{min}}}^{x_{\textrm{max}}} 
   = \int_{x_{0}}^{x_{N}} = \sum_{k=0}^{N-1} \int_{x_{k}}^{x_{k+1}}
\end{equation}
where all integrals share the same integrand as (\ref{eq:CONVDENS2}). Consider
the sequence $a_k$ and $b_k$ formed by the values of the densities $f_X(x)$ and
$f_Y(y)$ at grid points
\begin{equation}
  \label{eq:ABRECT}
    a_k = f_X(x_k), \quad b_k = f_Y(y_k) \qquad 0 \leqslant k \leqslant N-1
\end{equation}
with the convention $a_k$ and $b_k$ are zero when $k<0$ or $k>N-1$.  
Let $z_0 = x_0 + y_0$ and $z_n = z_0 +n \times h$ for integer $n$. Then $z_n
-x_k=y_{n-k}$ for all $n$, $k$.

\subsection{Rectangles}
%-----------------------
The rectangles approximation for~$f_Z(z)$ replaces each integral
$\int_{x_{k}}^{x_{k+1}}$ in~(\ref{eq:CONV0N}) by the product of the length $h =
x_{k+1} - x_k$ and the integrand at~$x_k$. This gives
\begin{equation}
  \label{eq:RECT}
   \int_{x_{k}}^{x_{k+1}}f_X(x)\,f_Y(z-x)\,\mathsf{d}x 
   \approx  h f_X(x_k)f_Y(z - x_k) 
\end{equation}
Replacing $z$ by $z_n$ and summing for $k=0$ to $k=N-1$, we get
\begin{equation*}
   \label{eq:APPROX}
  f_Z(z_n) \approx h \,\sum_{k=0}^{N-1} a_k b_{n-k}
  %% = h \, c_n
\end{equation*}
The sum at right hand has the same summand as the 
convolution product $c_n$, but the sum runs from $k=0$ to $n$ for $c_n$, against
$k=0$ to $N-1$ here. The two sums are identical for $0 \leqslant n \leqslant 2N-1$, i.e.
\begin{equation}
  \label{eq:IDENT}
  \sum_{k=0}^{N-1} a_k b_{n-k} = \sum_{k=0}^{n} a_k b_{n-k} \qquad \textrm{for}\quad 
  0 \leqslant n \leqslant 2N-1
\end{equation}
The reason is that $a_k$ and $b_{\ell}$ are zero when $[k,\,\ell]$ is outside
the square $0 \leqslant k,\,\ell \leqslant N-1$ (see figure~\ref{FIGCONV}).  
Note however that $f_Y(y_\ell)$ is only \textit{approximately zero} and
that the square side $Nh$ must be chosen with care, see~\ref{CHOOSEGRID}.

\subsection{Trapezoidal and  modification}
%-----------------------
The rectangles rule is known to be less precise than the trapezoidal rule which
has the same computational cost, and the later is always preferred. The integral
$\int_{x_{k}}^{x_{k+1}}$ in~(\ref{eq:CONV0N}) is then approximated by the
product of the length $h$ and the mean value of the integrand at the two
end-points~$x_k$ and $x_{k+1}$
\begin{equation}
  \label{eq:TRAP}
  \int_{x_{k}}^{x_{k+1}}f_X(x)\,f_Y(z-x)\,\mathsf{d}x 
  \approx \frac{h}{2} \left\{ f_X(x_k)f_Y(z - x_k) + f_X(x_{k+1})f_Y(z - x_{k+1}) \right\}
\end{equation}
Replacing $z$ by $z_n$, summing for $k=0$ to $k=N-1$ and using simple 
algebra we get
\begin{equation}
  \label{eq:TRAPFORM}
   f_Z(z_n) \approx \frac{1}{2h} \left\{ 2 \sum_{k=0}^{N-1} a_k b_{n-k} 
     - a_0 b_n + a_N b_{n-N}\right\}
\end{equation}
for $0 \leqslant n \leqslant N-1$. Since $a_0=0$ and $a_N=0$, the 
trapezoidal rule leads unsurprisingly to the same computation
as the rectangles rule.

%%\subsubsection{Modified trapezoidal}
%%-----------------------------
Rather than $b_k=f_Y(y_k)$, we can consider the mean value $b_k^\star$ of $f_Y(y)$ on the
interval~$(y_{k-1},\,y_{k})$
\begin{equation}
  \label{eq:AK}
  b_k^\star = \frac{1}{h}\,\int_{y_{k-1}}^{y_{k}} f_Y(y)\,\textrm{d}y = 
    \frac{1}{h}\,\left\{ S_Y(y_{k-1}) - S_Y(y_k) \right\}
\end{equation}
for $0 \leqslant k \leqslant N-1$, with the convention $S_Y(y_{k})=1$
for $k = -1$.  This choice uses the fact that $S_Y(y)$ is available
for exact computation, and ensures that the vector
$\m{b}^\star$ has unit sum. The mean value theorem tells that
$$
  \int_{x_{k}}^{x_{k+1}} f_X(x)\,f_Y(z_n-x)\,\mathsf{d}x 
  = f_X(\zeta_{n,k}) \times \int_{x_{k}}^{x_{k+1}} f_Y(z_n-x)\,\mathsf{d}x 
$$
for some $\zeta_{n,k}$ between $x_k$ and $x_{k+1}$. Actually, $f_X(x)$
is continuous and $f_Y(z-x)$ does not change of sign on the
interval. Thus
$$
\int_{x_{k}}^{x_{k+1}} f_X(x)\,f_Y(z_n-x)\,\mathsf{d}x =
f_X(\zeta_{n,k}) \times \int_{z_n-x_{k+1}}^{z_n-x_{k}}
f_Y(y)\,\mathsf{d}y = h \,f_X(\zeta_{n,k}) \,b_{n-k}^\star
$$
A reasonable approximation for the unknown $f_X(\zeta_{n,k})$ is the average
of the values of $f_X(x)$ at the two end-points $x_k$ and $x_{k+1}$.
This suggests the use of the following vectors 
\begin{equation}
  \label{eq:ABRECT2}
   a_k^\star = \frac{1}{2} \left\{f_X(x_k) + f_X(x_{k+1}) \right\}, \quad
   b_k^\star =  
    \frac{1}{h}\,\left\{ S_Y(y_{k-1}) - S_Y(y_k) \right\} \qquad 0 \leqslant k \leqslant N-1
\end{equation}
Then the approximation of the values $f_Z(z_n)$ is obtained by
discrete convolution as in the rectangular case, but using
$\m{a}^\star$ and $\m{b}^\star$ in place of $\m{a}$ and $\m{b}$.

Using some simple algebra, it can be shown that for $0\leqslant n \leqslant N-1$
\begin{equation}
  \label{eq:TRAPMFORM}
  \sum_{k=1}^{N-1} a_k^\star \,b_{n-k}^\star 
     = \frac{1}{2h} \,\left\{ 
       + \sum_{k=0}^{N-1} a_k \left[ B_{n-k-1} -B_{n-k+1} \right]
        -a_0 \left[ B_n-B_{n+1} \right]
       +a_N \left[ B_{n-N}-B_{n+1-N} \right] 
       \right\}
\end{equation}
where $B_k=S_Y(y_k)$ for $0\leqslant k \leqslant N-1$ (the sequence $B_k$ is
decreasing).  When the density $f_X(x)$ vanishes at its end-points, we have
$a_0=a_N=0$ and the convolution of $\m{a}^\star$ and $\m{b}^\star$ can be
computed as that of $\m{a}$ and $\m{b}^\dag$ with
\begin{equation}
  \label{eq:BDAG}
  b_k^\dag = \frac{1}{2h} \left\{S_Y(y_{k-1})- S_Y(y_{k+1})\right\} 
  \qquad 0 \leqslant k \leqslant N-1
\end{equation}
which is a centred difference approximation of $f_Y(y_k)$.  Note that the right
hand of the formula~(\ref{eq:TRAPMFORM}) is similar to that
of~(\ref{eq:TRAPFORM}), but each $b_k$ is replaced by a difference
approximation.


% One could alternatively use (\ref{eq:CONVSURV}), i.e. 
% $$
%  S_Z(z)
%    = \int_{x_{\textrm{min}}}^{x_{\textrm{max}}} f_X(x)\,S_Y(z_n-x)\,\textrm{d} x
% $$
% The computation is nearly unchanged: replacing the definition $b_\ell = f_Y(y_\ell)$
% by $b_\ell = S_Y(y_\ell)$. However, the relation $b_\ell = 0$ for $\ell \geqslant N$


\subsection*{Choosing grid limits}
%%------------------------------
\label{CHOOSEGRID}
While the support of $X$ is assumed to have finite bounds
$\Low{x}$ and $\Up{x}$, the support of $Y$ will in
most cases be infinite with $y_{\mathsf{max}}=+\infty$. Then we need
to fix a suitable finite upper limit $y_{\mathsf{max}}^\star$ in the
computations. The choice can be done using a small positive number
$\varepsilon_{\mathsf{max}}>0$ and solving
$S_Y(y_{\mathsf{max}}^\star) = \varepsilon_{\mathsf{max}}$.  Then
$S_Z(z)$ can be computed for $z \leqslant \Up{x} +
y_{\mathsf{max}}^\star$.  Since then $S_Z(\Up{x} +
y_{\mathsf{max}}^\star) \geqslant \varepsilon_{\mathsf{max}}$ the restriction on
return periods will be $T \leqslant 1/(\lambda \times
\varepsilon_{\mathsf{max}})$.

This strategy will be used even when the surge distribution has a
finite upper end-point $y_{\mathsf{max}}$, e.g. when $Y$ is GPD with
$\xi_Y <0$.  This allows the existence of an infinite density at the
upper end-point, e.g. for a GPD with $\xi_Y < -1$.

One may be concerned by the case where the density $f_Y(y)$ is continuous but
can be infinite at $y_{\mathsf{min}}$ as it is the case for the Weibull or gamma
distributions with decreasing hazards. Then as before, we replace
$y_{\mathsf{min}}$ by $y_{\mathsf{min}}^\star$ with $S_Y(y_{\mathsf{min}}^\star)
= 1-\varepsilon_{\mathsf{min}}$ and $\varepsilon_{\mathsf{min}}>0$ is chosen
small.

In the cases where the surge distribution has unbounded density either at
$y_{\mathsf{min}}$ or at $y_{\mathsf{max}}$, the results must be considered with
care.

\subsection*{Algorithm}
%%--------------------
\index{convolution!algorithm} $\varepsilon_{\mathsf{min}}$,
$\varepsilon_{\mathsf{max}}$ are chosen small positive real numbers and
$N$ is the chosen grid length.

\begin{enumerate}
  \item Let $H_X = \Up{x} - \Low{x}$. 
  \item Compute $y_{\mathsf{min}}^\star=q_Y(\varepsilon_{\mathsf{min}})$,
    $y_{\mathsf{max}}^\star=q_Y(1-\varepsilon_{\mathsf{max}})$ and
    $H_Y=y_{\mathsf{max}}^\star-y_{\mathsf{min}}^\star$.
  \item Let $H= \max(H_X,\,H_Y)$.
  \item Fix the grid step $h$ using $h = H/N$.
  \item Let $x_0 = \Low{x}$, $y_0 =  y_{\mathsf{min}}^\star$ and $z_0=x_0+y_0$.
  \item Fill the two vectors $\m{a}$ and $\m{b}$ of length $N$ with
    elements~(\ref{eq:ABRECT}), using linear interpolation for $\m{a}$.  
  \item Compute the convolution product vector $\m{c}$.
\end{enumerate}
The discrete convolution can rely on \verb@convolve@ from the \code{stats}
package.  This function uses the Fast Fourier Transform (FFT) and it is sound to
choose $N$ as a power of two i.e. $N=2^L$ with $L$ integer.

\begin{remarks}
\item The previous algorithm only describes the computation of $f_Z(z)$ 
  at grid values~$z_n$. Several results are obtained using a similar 
  convolution: approximated confidence limits (delta method), 
  conditional expectation $\Esp\pCond{X}{Z=z}$. See the code of the 
  function \verb@convSL@ for more details.

\item In the algorithm, the vectors $\m{a}$ and $\m{b}$ must be
  replaced by $\m{a}$ and $\m{b}^\dag$ of~(\ref{eq:BDAG}) to obtain
  the modified trapezoidal rule.  
  
\end{remarks}

\chapter{Validation and special cases}
%%----------------------------------
\section{Exponential surges}
%%--------------------------------
When a POT model with exponential distribution is used for the surge~$Y$,
the exact (conditional) distribution of $Z$ is known. More precisely,
conditional on $Z > x_{\mathrm{max}} + u$ the random variable $Z$ then
follows then an exponential distribution with shape $\sigma_Z=\sigma_Y$ and location
$\mu_Z$ given by 
\begin{equation*}
  \mu_Z = \mu_Y + \sigma_Y \log \Esp \left[e^{X/\sigma_Y}\right] 
\end{equation*}
where $\mu_Y$ and $\sigma_Y$ are the location and shape of~$Y$.

This result allows a simple check of the computation. The \verb@show.asympt@
argument of the \verb@convSL@ function allows us to add the theoretical
return level curve to the one computed by convolution. As an example, we can
use the computation for Brest. With the threshold $u=50$~cm, the surge
excesses can be considered as exponentially 
distributed with scale $\sigma_Y=10$~cm, i.e. with rate $1/10.0\,\mathrm{cm}^{-1}$.

<<label=BrestAsympt1, fig=TRUE, include=FALSE>>=
theta2.y <- c("rate" = 0.10)
conv.asympt <- convSL(dens.x = Brest.tide,
                      threshold.y = 50, 
                      distname.y = "exponential",
                      lambda = lambda, 
                      par.y = theta2.y, 
                      show.asympt = TRUE,
                      Tlim = c(5, 1000000),
                      main = "Asymptotic curve: exponential Y")
@ 
It can bee seen on the left panel of figure~\ref{ASYMPT}
that the return level curve computed by convolution nearly 
coincides with the exact result.

\section{GPD surges}
%%-----------------------
When a POT model with GPD distribution is used for the surge~$Y$,
the exact (conditional) distribution of $Z$ is no longer known,
but the asymptotic behaviour of the survival $S_Z(z)$ is known.

When $\xi_Y>0$ it can be shown that $S_Z(z)/S_Y(z)$ tends 
to~$1$, but with a \textit{very slow} convergence. The return
level curve should broadly behave as if $Z$ was GPD
with parameters $\mu_Z = \Esp(X) + \mu_Y$ (location),
$\sigma_Z = \sigma_Y$ (scale), and $\xi_Z = \xi_Y$ (shape).
<<label=BrestAsympt2, fig=TRUE, include=FALSE>>=
theta3.y <- c("scale" = 10, "shape" = 0.03)
conv.asympt <- convSL(dens.x = Brest.tide,
                      threshold.y = 50, 
                      distname.y = "GPD",
                      lambda = lambda, 
                      par.y = theta3.y, 
                      show.asympt = TRUE,
                      Tlim = c(5, 1000000),
                      main = sprintf("Asymptotic curve: GPD Y with shape %4.2f", 
                          theta3.y["shape"]))
@ 
The plot is on the right panel of figure~\ref{ASYMPT}. It suggests that
for \textit{very large return periods} the true curve could have a stronger
convexity than the curve computed by numerical convolution. 

\begin{figure}
  \centering
  \begin{tabular}{c c} 
    \includegraphics[width=5cm]{Rgraphics/fig-BrestAsympt1.pdf} &
    \includegraphics[width=5cm]{Rgraphics/fig-BrestAsympt2.pdf} 
    %% \includegraphics[width=7cm]{Rgraphics/fig-RenplotGaronne.pdf} 
  \end{tabular}
  \caption{\label{ASYMPT} Adding the asymptotic return level curve. Left panel: 
  exponential, right panel GPD with shape $\xi_Y>0$.}
\end{figure}

\bibliography{SeaLev}
\bibliographystyle{alpha}


\printindex
\end{document}

